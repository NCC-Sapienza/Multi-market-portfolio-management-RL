import numpy as np
import tqdm
import pandas as pd
import gurobipy as gp
import matplotlib.pyplot as plt
import yfinance as yf

import sys
import os
from multiprocessing import Pool
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from Actor_Critic_LSTM_NN import LSTM_model
from torch.utils.data import DataLoader
from utils import TimeSeriesDataset

class Env:
    """
    reset():                    Resets the environment to the initial state.
    step():                     Performs a step in the reinforced learning environment, taking an action, updating the state, and calculating the reward.

    this class implements an environment for reinforced learning with the goal of optimizing an agent's portfolio management. Decisions are made based on market predictions generated by a machine learning model.
    """
    def __init__(self, asset_dic, model):
        self.model          = model
        self.batch_size     = 32
        self.criterion      = nn.MSELoss()
        self.rolling_window = 10
        self.n_realiz       = 10
        self.pi             = 1 / self.n_realiz
        self.delta          = 0.2
        self.beta           = 1   #TODO: da verificare
        self.alpha          = .1   #TODO: da verificare
        self.M              = range(len(asset_dic))
        self.N_k            = range(len(asset_dic[0]))
        self.S              = range(self.n_realiz)
        self.action_space   = len(asset_dic) * len(asset_dic[0])
        self.tickers        = asset_dic
        self.pre_state      = np.zeros(self.action_space)
        self.old_prices     = np.zeros(self.action_space)
        self.max_iterations = 770

        self.budget         = 1000
        self.contatore      = 0
        self.load_data()

    def load_data(self):

        self.data = {}
        self.og_data = {}
        for market in self.M:
            for ticker in self.tickers[market]:
                OG_dataset = pd.read_excel(f"Dataset//{ticker}_dataset.xlsx")
                OG_dataset = OG_dataset.drop("Date", axis=1)
                col = OG_dataset.columns
                scaler = MinMaxScaler()
                dataset = pd.DataFrame(scaler.fit_transform(OG_dataset), columns=col)
                self.data[ticker] = dataset
                self.og_data[ticker] = OG_dataset
        print("fatto")



    def reset(self):
        self.budget = 1000
        self.pre_state = np.zeros(self.action_space)
        self.contatore = 0

        return self.pre_state




    def create_sequences(self, data, seq_length):
        data = data.reset_index(drop=True)
        xs = []
        ys = []
        for i in (range(len(data) - seq_length)):
            x = data[i:(i + seq_length)]
            y = data['Close'][i + seq_length]
            xs.append(x)
            ys.append(y)
        return np.array(xs), np.array(ys)


    def std_dev(self, real_data, pred_data):
        """
        :param real_data: Real data values
        :param pred_data: Predicted data values
        :return: White noise based on the standard deviation of the last 10 data points
        """

        data = np.concatenate((real_data, pred_data))
        last_100_data = data[-100:]
        std_dev = np.std(last_100_data)

        return std_dev


    def step(self, action):

        predizioni = []
        close_prices = []
        Real_prices = []
        for market in self.M:
            for ticker in self.tickers[market]:
                #print(f"\n\nProcessing {ticker}")

                try:
                    self.model.load_state_dict(torch.load(f"models pth\\best_{ticker}_model.pth"))
                except FileNotFoundError:
                    print("File not found.")
                    break

                dataset = self.data[ticker]
                OG_dataset = self.og_data[ticker]


                test_size = round(3861 * 0.2)
                test_dataset = dataset.tail(test_size).reset_index(drop=True)
                OG_test_dataset = OG_dataset.tail(test_size).reset_index(drop=True)

                xs, ys = self.create_sequences(test_dataset, self.rolling_window)
                dataset = TimeSeriesDataset(xs, ys)
                dataloader = DataLoader(dataset, batch_size=32, shuffle=False)

                data_std_dev = test_dataset["Close"].iloc[-100:].to_numpy()
                data_torch = torch.tensor(test_dataset.iloc[self.contatore:(self.contatore + self.rolling_window), :].to_numpy(), dtype=torch.float).unsqueeze(0)

                close_prices.append(test_dataset["Close"].iloc[self.contatore + self.rolling_window])
                Real_prices.append(1/OG_test_dataset["Close"].iloc[self.contatore + self.rolling_window])

                pred_list = []
                self.model.eval().cpu()
                with torch.no_grad():
                    for batch in dataloader:
                        x,y = batch

                        pred= self.model(x)
                        pred_list.append(pred.cpu())

                    pred_list = torch.cat(pred_list, dim=0)

                    std_dev = self.std_dev(data_std_dev, pred_list)
                    pred = self.model(data_torch)
                    for _ in range(10):
                        noise = np.random.normal(0, std_dev, pred.shape)
                        pred_noise = np.array(pred) + noise
                        predizioni.append(pred_noise)



        predizioni = np.array(predizioni).reshape(10,20).T


        close_prices_expanded = np.expand_dims(np.array(close_prices), axis=1)
        R_mod = (predizioni - close_prices_expanded) / close_prices_expanded

        R = np.array(R_mod).reshape(4, 5, 10).T
        r_hat = np.sum(R, axis=0) * self.pi
        r_hat = r_hat * action.reshape(len(self.M), len(self.N_k)).T

        # Gurobi model
        G_model = gp.Model()

        # Gurobi var
        x = G_model.addVars(self.N_k, self.M, vtype=gp.GRB.CONTINUOUS, name="x")
        z = G_model.addVars(self.M, vtype=gp.GRB.CONTINUOUS, name="z")
        eta = G_model.addVars(self.M, vtype=gp.GRB.CONTINUOUS, lb=-10e9, name="eta")
        theta = G_model.addVar(vtype=gp.GRB.CONTINUOUS, name="theta")
        w = G_model.addVars(self.S, self.M, vtype=gp.GRB.CONTINUOUS, name="w")
        G_model.update()

        # Gurobi obj fun
        quicksum = gp.quicksum(x[i, k] * r_hat[i, k] for i in self.N_k for k in self.M)
        G_model.setObjective((self.beta * quicksum) - ((1 - self.beta) * theta), gp.GRB.MAXIMIZE)

        # Gurobi Constraints
        G_model.addConstrs((gp.quicksum(x[i, k] for i in self.N_k) <= z[k] for k in self.M), name="10b")
        G_model.addConstr(gp.quicksum(z[k] for k in self.M) <= 1, name="10c")
        G_model.addConstrs(
            (eta[k] + (1 - self.alpha) ** (-1) * gp.quicksum(self.pi * w[j, k] for j in self.S) <= theta for k
             in self.M), name="10d")
        G_model.addConstrs(
            (w[j, k] >= z[k] - gp.quicksum(R[j, i, k] * x[i, k] for i in self.N_k) - eta[k] for j in self.S for
             k in self.M), name="10e")  # modifica il calcolo R*x
        G_model.addConstrs((x[i, k] >= 0 for i in self.N_k for k in self.M), name="10f")
        G_model.addConstrs((w[j, k] >= 0 for j in self.S for k in self.M), name="10g")
        G_model.addConstrs((z[k] >= 0 for k in self.M), name="10h")
        G_model.addConstr(theta >= 0, name="10i")
        G_model.update()  # stampa il modello

        # Gurobi Solver
        G_model.optimize()

        x_values = np.empty((len(self.N_k), len(self.M)))
        for i in range(len(self.N_k)):
            for k in range(len(self.M)):
                x_values[i, k] = x[i, k].X

        state = (np.array(x_values).T.reshape(len(self.N_k) * len(self.M)) * self.budget) * (
            np.array(Real_prices))

        if self.contatore == 0:
            reward = 0
            self.pre_state = state
            self.old_prices = Real_prices
        else:

            reward = (np.sum(self.pre_state * Real_prices) - np.sum(self.pre_state * self.old_prices)) / np.sum(
                self.pre_state * self.old_prices)
            self.pre_state = state
            self.old_prices = Real_prices
            self.budget = self.budget * (1 + reward)

        if self.budget <= 500:
            done = True
            info = f"Reached minimum budget... budget: {self.budget}"

        elif self.contatore >= self.max_iterations:
            done = True
            info = f"End of test... budget: {self.budget}"
        else:
            done = False
            info = f"proceeding... budget: {self.budget}"
        self.contatore += 1


        print(self.budget)

        return state , reward, done, info



ASSETS = {
    0   : ["7203.T", "9984.T", "6758.T", "6861.T", "9983.T"],
    1   : ["BRK-A", "JNJ", "PG", "V", "JPM"],
    2   : ["AAPL", "AMZN", "MSFT", "GOOGL", "NVDA"],
    3   : ["ULVR.L", "HSBA.L", "BATS.L", "DGE.L", "BP.L"]
}























